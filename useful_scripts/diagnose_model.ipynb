{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4aab641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo root: /home/wdy02/wdy_program/simulation_plus/IsaacLab\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146965/4229847810.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 324 个完全匹配的参数。\n",
      "state/model 匹配项: 324, 不匹配项: 6 (展示前10项)\n",
      "  mismatch: ('fusion.2.weight', torch.Size([512, 512]), None)\n",
      "  mismatch: ('fusion.2.bias', torch.Size([512]), None)\n",
      "  mismatch: ('fusion.4.weight', torch.Size([256, 512]), None)\n",
      "  mismatch: ('fusion.4.bias', torch.Size([256]), None)\n",
      "  mismatch: ('fusion.6.weight', torch.Size([7, 256]), torch.Size([256, 512]))\n",
      "  mismatch: ('fusion.6.bias', torch.Size([7]), torch.Size([256]))\n",
      "Loaded scaler from /home/wdy02/wdy_program/simulation_plus/IsaacLab/wdy_data/bc_data/bc_ckpts/ONE_PEG_IN_HOLE_60/scaler.pkl\n",
      "从 npz 读取到 1 个样本（优先使用）。\n",
      "\n",
      "--- sample 1 ---\n",
      "输入形状: front (1, 3, 128, 128) hand (1, 3, 128, 128) proprio (1, 78)\n",
      "  front NaN: False, Inf: False, mean 1.4874e+02, min 0.0000e+00, max 2.5500e+02\n",
      "  hand NaN: False, Inf: False, mean 1.7075e+02, min 1.0000e+00, max 2.5400e+02\n",
      "  proprio NaN: False, Inf: False, mean 1.6170e-01, min -2.8698e+00, max 3.2277e+00\n",
      "=== forward 输出统计 ===\n",
      "raw pre-tanh min/max/mean: -0.04922686889767647 0.046364836394786835 0.002185981022194028\n",
      "post-tanh min/max/mean: -0.04918714240193367 0.04633164033293724 0.002186789410188794\n",
      "last_linear weight mean/min/max: -0.000238517633988522 -0.06247566640377045 0.062497951090335846\n",
      "last_linear bias mean/min/max: 0.0026842153165489435 -0.052161239087581635 0.04825769364833832\n",
      "img feat f1 mean/min/max: 1.8923835111474492e-40 0.0 2.221170169831981e-40\n",
      "img feat f2 mean/min/max: 1.8923835111474492e-40 0.0 2.221170169831981e-40\n",
      "proprio (raw) stats min/max/mean: -2.869821310043335 3.2277328968048096 0.16170240938663483\n",
      "mapped action A (env-range): [-0.00855577  0.00986147 -0.04918712 -0.02635676  0.01982808  0.04633164\n",
      "  0.023386  ]\n",
      "mapped action B (sym-debug): [-0.00855578  0.00986148 -0.04918714 -0.02635677  0.0198281   0.04633164\n",
      "  0.02338599]\n",
      "模型层统计（线性/conv/bn）:\n",
      "  Conv2d img_enc.backbone.0: weight (64, 3, 7, 7), mean -1.3621e-42\n",
      "  BN img_enc.backbone.1\n",
      "  Conv2d img_enc.backbone.4.0.conv1: weight (64, 64, 1, 1), mean 4.1198e-43\n",
      "  BN img_enc.backbone.4.0.bn1\n",
      "  Conv2d img_enc.backbone.4.0.conv2: weight (64, 64, 3, 3), mean -5.7453e-44\n",
      "  BN img_enc.backbone.4.0.bn2\n",
      "  Conv2d img_enc.backbone.4.0.conv3: weight (256, 64, 1, 1), mean -2.0179e-43\n",
      "  BN img_enc.backbone.4.0.bn3\n",
      "  Conv2d img_enc.backbone.4.0.downsample.0: weight (256, 64, 1, 1), mean -2.9848e-43\n",
      "  BN img_enc.backbone.4.0.downsample.1\n",
      "  Conv2d img_enc.backbone.4.1.conv1: weight (64, 256, 1, 1), mean 9.2486e-44\n",
      "  BN img_enc.backbone.4.1.bn1\n",
      "  Conv2d img_enc.backbone.4.1.conv2: weight (64, 64, 3, 3), mean -7.8473e-44\n",
      "  BN img_enc.backbone.4.1.bn2\n",
      "  Conv2d img_enc.backbone.4.1.conv3: weight (256, 64, 1, 1), mean -6.1377e-43\n",
      "  BN img_enc.backbone.4.1.bn3\n",
      "  Conv2d img_enc.backbone.4.2.conv1: weight (64, 256, 1, 1), mean 2.5784e-43\n",
      "  BN img_enc.backbone.4.2.bn1\n",
      "  Conv2d img_enc.backbone.4.2.conv2: weight (64, 64, 3, 3), mean 7.5670e-43\n",
      "  BN img_enc.backbone.4.2.bn2\n",
      "  Conv2d img_enc.backbone.4.2.conv3: weight (256, 64, 1, 1), mean 6.1797e-43\n",
      "  BN img_enc.backbone.4.2.bn3\n",
      "  Conv2d img_enc.backbone.5.0.conv1: weight (128, 256, 1, 1), mean 2.6204e-43\n",
      "  BN img_enc.backbone.5.0.bn1\n",
      "  Conv2d img_enc.backbone.5.0.conv2: weight (128, 128, 3, 3), mean -7.8473e-44\n",
      "  BN img_enc.backbone.5.0.bn2\n",
      "  Conv2d img_enc.backbone.5.0.conv3: weight (512, 128, 1, 1), mean 5.9275e-43\n",
      "  BN img_enc.backbone.5.0.bn3\n",
      "  Conv2d img_enc.backbone.5.0.downsample.0: weight (512, 256, 1, 1), mean 1.4714e-43\n",
      "  BN img_enc.backbone.5.0.downsample.1\n",
      "  Conv2d img_enc.backbone.5.1.conv1: weight (128, 512, 1, 1), mean 9.1785e-43\n",
      "  BN img_enc.backbone.5.1.bn1\n",
      "  Conv2d img_enc.backbone.5.1.conv2: weight (128, 128, 3, 3), mean -2.6064e-43\n",
      "  BN img_enc.backbone.5.1.bn2\n",
      "  Conv2d img_enc.backbone.5.1.conv3: weight (512, 128, 1, 1), mean 7.7492e-43\n",
      "  BN img_enc.backbone.5.1.bn3\n",
      "  Conv2d img_enc.backbone.5.2.conv1: weight (128, 512, 1, 1), mean 6.0816e-43\n",
      "  BN img_enc.backbone.5.2.bn1\n",
      "  Conv2d img_enc.backbone.5.2.conv2: weight (128, 128, 3, 3), mean 1.9618e-43\n",
      "  BN img_enc.backbone.5.2.bn2\n",
      "  Conv2d img_enc.backbone.5.2.conv3: weight (512, 128, 1, 1), mean 2.1860e-43\n",
      "  BN img_enc.backbone.5.2.bn3\n",
      "  Conv2d img_enc.backbone.5.3.conv1: weight (128, 512, 1, 1), mean 1.2920e-42\n",
      "  BN img_enc.backbone.5.3.bn1\n",
      "  Conv2d img_enc.backbone.5.3.conv2: weight (128, 128, 3, 3), mean 3.0688e-43\n",
      "  BN img_enc.backbone.5.3.bn2\n",
      "  Conv2d img_enc.backbone.5.3.conv3: weight (512, 128, 1, 1), mean 4.8485e-43\n",
      "  BN img_enc.backbone.5.3.bn3\n",
      "  Conv2d img_enc.backbone.6.0.conv1: weight (256, 512, 1, 1), mean 5.8448e-42\n",
      "  BN img_enc.backbone.6.0.bn1\n",
      "  Conv2d img_enc.backbone.6.0.conv2: weight (256, 256, 3, 3), mean 1.5835e-43\n",
      "  BN img_enc.backbone.6.0.bn2\n",
      "  Conv2d img_enc.backbone.6.0.conv3: weight (1024, 256, 1, 1), mean 8.0435e-43\n",
      "  BN img_enc.backbone.6.0.bn3\n",
      "  Conv2d img_enc.backbone.6.0.downsample.0: weight (1024, 512, 1, 1), mean 9.6690e-44\n",
      "  BN img_enc.backbone.6.0.downsample.1\n",
      "  Conv2d img_enc.backbone.6.1.conv1: weight (256, 1024, 1, 1), mean 2.1019e-42\n",
      "  BN img_enc.backbone.6.1.bn1\n",
      "  Conv2d img_enc.backbone.6.1.conv2: weight (256, 256, 3, 3), mean 4.3440e-43\n",
      "  BN img_enc.backbone.6.1.bn2\n",
      "  Conv2d img_enc.backbone.6.1.conv3: weight (1024, 256, 1, 1), mean 2.7465e-43\n",
      "  BN img_enc.backbone.6.1.bn3\n",
      "  Conv2d img_enc.backbone.6.2.conv1: weight (256, 1024, 1, 1), mean 2.7606e-42\n",
      "  BN img_enc.backbone.6.2.bn1\n",
      "  Conv2d img_enc.backbone.6.2.conv2: weight (256, 256, 3, 3), mean 1.1925e-42\n",
      "  BN img_enc.backbone.6.2.bn2\n",
      "  Conv2d img_enc.backbone.6.2.conv3: weight (1024, 256, 1, 1), mean 4.0077e-43\n",
      "  BN img_enc.backbone.6.2.bn3\n",
      "  Conv2d img_enc.backbone.6.3.conv1: weight (256, 1024, 1, 1), mean 2.1482e-42\n",
      "  BN img_enc.backbone.6.3.bn1\n",
      "  Conv2d img_enc.backbone.6.3.conv2: weight (256, 256, 3, 3), mean 3.2931e-43\n",
      "  BN img_enc.backbone.6.3.bn2\n",
      "  Conv2d img_enc.backbone.6.3.conv3: weight (1024, 256, 1, 1), mean 5.2689e-43\n",
      "  BN img_enc.backbone.6.3.bn3\n",
      "  Conv2d img_enc.backbone.6.4.conv1: weight (256, 1024, 1, 1), mean 2.9119e-42\n",
      "  BN img_enc.backbone.6.4.bn1\n",
      "  Conv2d img_enc.backbone.6.4.conv2: weight (256, 256, 3, 3), mean 1.6816e-43\n",
      "  BN img_enc.backbone.6.4.bn2\n",
      "  Conv2d img_enc.backbone.6.4.conv3: weight (1024, 256, 1, 1), mean 7.4829e-43\n",
      "  BN img_enc.backbone.6.4.bn3\n",
      "  Conv2d img_enc.backbone.6.5.conv1: weight (256, 1024, 1, 1), mean 1.9716e-42\n",
      "  BN img_enc.backbone.6.5.bn1\n",
      "  Conv2d img_enc.backbone.6.5.conv2: weight (256, 256, 3, 3), mean 2.7045e-43\n",
      "  BN img_enc.backbone.6.5.bn2\n",
      "  Conv2d img_enc.backbone.6.5.conv3: weight (1024, 256, 1, 1), mean 8.8702e-43\n",
      "  BN img_enc.backbone.6.5.bn3\n",
      "  Conv2d img_enc.backbone.7.0.conv1: weight (512, 1024, 1, 1), mean 3.5117e-42\n",
      "  BN img_enc.backbone.7.0.bn1\n",
      "  Conv2d img_enc.backbone.7.0.conv2: weight (512, 512, 3, 3), mean 9.8091e-44\n",
      "  BN img_enc.backbone.7.0.bn2\n",
      "  Conv2d img_enc.backbone.7.0.conv3: weight (2048, 512, 1, 1), mean 6.9350e-42\n",
      "  BN img_enc.backbone.7.0.bn3\n",
      "  Conv2d img_enc.backbone.7.0.downsample.0: weight (2048, 1024, 1, 1), mean 2.5083e-42\n",
      "  BN img_enc.backbone.7.0.downsample.1\n",
      "  Conv2d img_enc.backbone.7.1.conv1: weight (512, 2048, 1, 1), mean 9.8371e-43\n",
      "  BN img_enc.backbone.7.1.bn1\n",
      "  Conv2d img_enc.backbone.7.1.conv2: weight (512, 512, 3, 3), mean -3.6434e-44\n",
      "  BN img_enc.backbone.7.1.bn2\n",
      "  Conv2d img_enc.backbone.7.1.conv3: weight (2048, 512, 1, 1), mean 3.2370e-42\n",
      "  BN img_enc.backbone.7.1.bn3\n",
      "  Conv2d img_enc.backbone.7.2.conv1: weight (512, 2048, 1, 1), mean 1.4461e-42\n",
      "  BN img_enc.backbone.7.2.bn1\n",
      "  Conv2d img_enc.backbone.7.2.conv2: weight (512, 512, 3, 3), mean -1.2051e-43\n",
      "  BN img_enc.backbone.7.2.bn2\n",
      "  Conv2d img_enc.backbone.7.2.conv3: weight (2048, 512, 1, 1), mean 3.0310e-42\n",
      "  BN img_enc.backbone.7.2.bn3\n",
      "  Linear pro_enc.net.0: weight (256, 78), mean 5.8796e-04, min -9.6135e-02, max 1.4815e-01\n",
      "    bias mean 2.8021e-03, min -8.9480e-03, max 1.1048e-01\n",
      "  Linear pro_enc.net.2: weight (256, 256), mean -6.5223e-05, min -5.8043e-02, max 9.0632e-02\n",
      "    bias mean 2.4903e-03, min -6.0591e-41, max 4.2767e-02\n",
      "  Linear fusion.0: weight (512, 4352), mean 9.3669e-06, min -6.3058e-41, max 7.4368e-02\n",
      "    bias mean -4.0524e-05, min -5.9972e-03, max 2.7049e-04\n",
      "  Linear fusion.2: weight (512, 512), mean -2.0292e-05, min -4.4194e-02, max 4.4194e-02\n",
      "    bias mean -1.9364e-03, min -4.3761e-02, max 4.3858e-02\n",
      "  Linear fusion.4: weight (256, 512), mean -4.3833e-05, min -4.4194e-02, max 4.4194e-02\n",
      "    bias mean -5.3647e-04, min -4.4169e-02, max 4.4014e-02\n",
      "  Linear fusion.6: weight (7, 256), mean -2.3852e-04, min -6.2476e-02, max 6.2498e-02\n",
      "    bias mean 2.6842e-03, min -5.2161e-02, max 4.8258e-02\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Enhanced BC model diagnostic script.\n",
    "Features:\n",
    " - robust checkpoint loading and partial-parameter copying\n",
    " - detailed layer-by-layer statistics\n",
    " - auto-insert a proprio adapter linear when input dim mismatch detected (fixes mat1/mat2 shape error)\n",
    " - supports npz samples / env / fallback random obs\n",
    " - scaler support (Standardizer.load if available)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- repo root detection (兼容 notebook/script) ----------\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).resolve().parents[3]\n",
    "except NameError:\n",
    "    REPO_ROOT = Path.cwd().resolve().parents[1]\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(f\"Using repo root: {REPO_ROOT}\")\n",
    "\n",
    "# ---------- try imports from project ----------\n",
    "BCPolicy = None\n",
    "FrankaGym = None\n",
    "Standardizer = None\n",
    "try:\n",
    "    from wdy_file.useful_scripts.bc_pipeline import BCPolicy, Standardizer  # type: ignore\n",
    "    Standardizer = Standardizer\n",
    "except Exception as e:\n",
    "    try:\n",
    "        from wdy_file.useful_scripts.bc_pipeline import BCPolicy  # type: ignore\n",
    "    except Exception as e2:\n",
    "        BCPolicy = None\n",
    "        print(\"警告：无法导入 BCPolicy:\", e2)\n",
    "\n",
    "try:\n",
    "    from wdy_file.wdy_assemble_gym import FrankaGym\n",
    "except Exception:\n",
    "    FrankaGym = None\n",
    "\n",
    "# ---------- checkpoint loader ----------\n",
    "def load_checkpoint(ckpt_path, device):\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"checkpoint not found: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = None\n",
    "    model_kwargs = {}\n",
    "    if isinstance(ckpt, dict):\n",
    "        # try common keys\n",
    "        state = ckpt.get('model') or ckpt.get('model_state_dict') or ckpt.get('state_dict') or None\n",
    "        model_kwargs = ckpt.get('model_kwargs', {}) or {}\n",
    "    else:\n",
    "        state = ckpt\n",
    "    # strip \"module.\" prefix if present\n",
    "    if isinstance(state, dict) and any(k.startswith(\"module.\") for k in state.keys()):\n",
    "        new = {}\n",
    "        for k,v in state.items():\n",
    "            nk = k[len(\"module.\"): ] if k.startswith(\"module.\") else k\n",
    "            new[nk] = v\n",
    "        state = new\n",
    "    return state, model_kwargs, ckpt\n",
    "\n",
    "# ---------- helper: summary of state dict vs model dict ----------\n",
    "def compare_state_and_model(state_dict, model):\n",
    "    ms = model.state_dict()\n",
    "    matches = []\n",
    "    mismatches = []\n",
    "    for k_m, v_m in ms.items():\n",
    "        if state_dict is None:\n",
    "            mismatches.append((k_m, v_m.shape, None))\n",
    "            continue\n",
    "        v = state_dict.get(k_m, None)\n",
    "        if v is None:\n",
    "            mismatches.append((k_m, v_m.shape, None))\n",
    "        else:\n",
    "            if v.shape == v_m.shape:\n",
    "                matches.append((k_m, v_m.shape))\n",
    "            else:\n",
    "                mismatches.append((k_m, v_m.shape, v.shape))\n",
    "    return matches, mismatches\n",
    "\n",
    "# ---------- build model and attempt robust load ----------\n",
    "def build_model_from_kwargs(model_kwargs, state_dict, device):\n",
    "    if BCPolicy is None:\n",
    "        raise RuntimeError(\"BCPolicy class not available for constructing model.\")\n",
    "    if not model_kwargs:\n",
    "        raise RuntimeError(\"checkpoint 缺少 model_kwargs，请提供训练时的 model 参数（input_dim/output_dim/hidden_fusion 等）\")\n",
    "    # map known keys\n",
    "    input_dim = model_kwargs.get('input_dim') or model_kwargs.get('proprio_dim') or model_kwargs.get('proprio_input_dim') or None\n",
    "    output_dim = model_kwargs.get('output_dim') or model_kwargs.get('action_dim') or 7\n",
    "    hidden_fusion = model_kwargs.get('hidden_fusion', (512,512,256))\n",
    "\n",
    "    if input_dim is None:\n",
    "        print(\"警告：无法从 model_kwargs 推断 input_dim/proprio_dim，继续构造模型但后续可能需要 adapter。\")\n",
    "        input_dim = 10  # placeholder\n",
    "\n",
    "    # construct model (try-catch)\n",
    "    model = BCPolicy(input_dim=int(input_dim), hidden_fusion=hidden_fusion, output_dim=int(output_dim))\n",
    "    model.to(device)\n",
    "\n",
    "    if state_dict is not None:\n",
    "        ms = model.state_dict()\n",
    "        # exact matches\n",
    "        load_keys = {k: v for k, v in state_dict.items() if (k in ms and v.shape == ms[k].shape)}\n",
    "        if load_keys:\n",
    "            ms.update(load_keys)\n",
    "            model.load_state_dict(ms, strict=False)\n",
    "            print(f\"加载 {len(load_keys)} 个完全匹配的参数。\")\n",
    "        else:\n",
    "            # partial copy\n",
    "            partial = {}\n",
    "            for k,v in state_dict.items():\n",
    "                if k in ms:\n",
    "                    v_m = ms[k]\n",
    "                    if v.ndim == v_m.ndim:\n",
    "                        mins = tuple(min(a,b) for a,b in zip(v.shape, v_m.shape))\n",
    "                        if any(m==0 for m in mins):\n",
    "                            continue\n",
    "                        new = v_m.clone()\n",
    "                        slices = tuple(slice(0,m) for m in mins)\n",
    "                        new[slices] = v[slices]\n",
    "                        partial[k] = new\n",
    "            if partial:\n",
    "                ms.update(partial)\n",
    "                model.load_state_dict(ms, strict=False)\n",
    "                print(f\"部分拷贝 {len(partial)} 参数（重叠区域）。\")\n",
    "            else:\n",
    "                print(\"未加载 checkpoint 参数（找不到匹配形状），返回随机初始化模型。\")\n",
    "    # show comparison\n",
    "    if state_dict is not None:\n",
    "        matches, mismatches = compare_state_and_model(state_dict, model)\n",
    "        print(f\"state/model 匹配项: {len(matches)}, 不匹配项: {len(mismatches)} (展示前10项)\")\n",
    "        for m in mismatches[:10]:\n",
    "            print(\"  mismatch:\", m)\n",
    "    return model\n",
    "\n",
    "# ---------- wrapper model that applies adapter to proprio input ----------\n",
    "class ModelWithAdapter(nn.Module):\n",
    "    def __init__(self, model, adapter: nn.Module, adapter_name=\"proprio_adapter\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.adapter = adapter\n",
    "        self.adapter_name = adapter_name\n",
    "\n",
    "    def forward(self, front, hand, proprio):\n",
    "        # allow numpy / tensor\n",
    "        if isinstance(proprio, np.ndarray):\n",
    "            proprio = torch.tensor(proprio, dtype=torch.float32, device=next(self.model.parameters()).device)\n",
    "        # if adapter exists, apply it\n",
    "        if self.adapter is not None:\n",
    "            # adapter expects shape (B, D)\n",
    "            proprio = self.adapter(proprio)\n",
    "        return self.model(front, hand, proprio)\n",
    "\n",
    "# ---------- try to auto-wrap with adapter if forward fails due to matmul shape ----------\n",
    "def wrap_model_with_adapter_if_needed(model, sample_front, sample_hand, sample_proprio, device):\n",
    "    \"\"\"\n",
    "    Try a forward pass. If it throws a matmul shape mismatch that indicates\n",
    "    proprio length != model expected; infer expected_in from first Linear layer\n",
    "    (heuristic) and build adapter: nn.Linear(actual_proprio_dim, expected_in).\n",
    "    Returns (model_maybe_wrapped, adapter_created_flag).\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    sample_front = sample_front.to(device)\n",
    "    sample_hand = sample_hand.to(device)\n",
    "    sample_proprio = sample_proprio.to(device)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = model(sample_front, sample_hand, sample_proprio)\n",
    "        return model, False\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e)\n",
    "        print(\"初次前向时捕获 RuntimeError:\", msg)\n",
    "        # try to detect matmul shape error\n",
    "        if \"mat1 and mat2 shapes cannot be multiplied\" in msg or \"mat1 and mat2\" in msg:\n",
    "            # infer actual proprio dim\n",
    "            actual_dim = int(sample_proprio.shape[1])\n",
    "            # heuristic: find a linear layer whose in_features is small (likely expects proprio)\n",
    "            candidate_in = None\n",
    "            for name, m in model.named_modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    in_f = m.weight.shape[1]\n",
    "                    # prefer smaller in_f relative to actual_dim, or first found\n",
    "                    if candidate_in is None or abs(in_f - actual_dim) < abs(candidate_in - actual_dim):\n",
    "                        candidate_in = int(in_f)\n",
    "                        candidate_name = name\n",
    "            if candidate_in is None:\n",
    "                print(\"无法找到线性层来推断期望的 proprio 维度，请手动检查模型结构。\")\n",
    "                raise\n",
    "            print(f\"检测到实际 proprio dim={actual_dim}，模型可能期望 dim={candidate_in}（推断于层 '{candidate_name}'）。\")\n",
    "            # create adapter\n",
    "            adapter = nn.Linear(actual_dim, candidate_in)\n",
    "            # init adapter weights sensibly (small)\n",
    "            nn.init.xavier_uniform_(adapter.weight, gain=0.01)\n",
    "            if adapter.bias is not None:\n",
    "                nn.init.constant_(adapter.bias, 0.0)\n",
    "            adapter = adapter.to(device)\n",
    "            wrapped = ModelWithAdapter(model, adapter)\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _ = wrapped(sample_front, sample_hand, sample_proprio)\n",
    "                print(\"已插入 proprio adapter 并成功前向。\")\n",
    "                return wrapped, True\n",
    "            except Exception as e2:\n",
    "                print(\"插入 adapter 后仍然失败，错误：\", e2)\n",
    "                raise\n",
    "        else:\n",
    "            # other runtime error: re-raise\n",
    "            raise\n",
    "\n",
    "# ---------- preprocessing obs (mostly from your code, enhanced) ----------\n",
    "def preprocess_obs_for_model(obs, device, scaler=None, target_img_hw=(128,128)):\n",
    "    # 取第一帧\n",
    "    arr = obs['front_rgb'][0]  # shape (N,128,128,3)\n",
    "    front_camera_rgb = torch.tensor(arr,dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "    arr = obs['wrist_rgb'][0]  # shape (N, 128,128,3)\n",
    "    hand_camera_rgb = torch.tensor(arr,dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "    # 取 proprio obs\n",
    "    # 1. 按训练时处理观测\n",
    "    for k,v in obs.items():\n",
    "        if k.endswith('contact_forces'):\n",
    "            contact_forces = v\n",
    "\n",
    "    proprio_vec = np.concatenate([\n",
    "        obs['ee_pose'][0],\n",
    "        obs['joint_states'][0],\n",
    "        obs['joint_forces'][0],\n",
    "        contact_forces[0]\n",
    "    ],axis=0)\n",
    "    proprio_vec = torch.tensor(proprio_vec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    proprio_scaled = scaler.transform(proprio_vec)\n",
    "    return front_camera_rgb, hand_camera_rgb, proprio_scaled\n",
    "\n",
    "# ---------- enhanced layer stats helper ----------\n",
    "def print_model_stats(model):\n",
    "    print(\"模型层统计（线性/conv/bn）:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            w = m.weight.detach().cpu().numpy()\n",
    "            b = m.bias.detach().cpu().numpy() if m.bias is not None else None\n",
    "            print(f\"  Linear {name}: weight {w.shape}, mean {w.mean():.4e}, min {w.min():.4e}, max {w.max():.4e}\")\n",
    "            if b is not None:\n",
    "                print(f\"    bias mean {b.mean():.4e}, min {b.min():.4e}, max {b.max():.4e}\")\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            w = m.weight.detach().cpu().numpy()\n",
    "            print(f\"  Conv2d {name}: weight {w.shape}, mean {w.mean():.4e}\")\n",
    "        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "            print(f\"  BN {name}\")\n",
    "\n",
    "# ---------- main diagnostic pass ----------\n",
    "def diagnose_once(model, obs, device, scaler=None, env=None):\n",
    "    model.eval()\n",
    "    front_t, hand_t, proprio_t = preprocess_obs_for_model(obs, device, scaler)\n",
    "    # quick checks input shapes\n",
    "    print(\"输入形状: front\", tuple(front_t.shape), \"hand\", tuple(hand_t.shape), \"proprio\", tuple(proprio_t.shape))\n",
    "    # check NaN/Inf in inputs\n",
    "    for name, t in [('front', front_t), ('hand', hand_t), ('proprio', proprio_t)]:\n",
    "        arr = t.cpu().numpy()\n",
    "        print(f\"  {name} NaN: {np.isnan(arr).any()}, Inf: {np.isinf(arr).any()}, mean {arr.mean():.4e}, min {arr.min():.4e}, max {arr.max():.4e}\")\n",
    "    # try forward (model may be wrapped already)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            out = model(front_t, hand_t, proprio_t)\n",
    "    except RuntimeError as e:\n",
    "        print(\"首次前向抛出错误：\", e)\n",
    "        raise\n",
    "    if isinstance(out, torch.Tensor):\n",
    "        out_np = out.cpu().numpy()[0]\n",
    "    else:\n",
    "        out_np = np.asarray(out)[0]\n",
    "    print(\"=== forward 输出统计 ===\")\n",
    "    print(\"raw pre-tanh min/max/mean:\", float(out_np.min()), float(out_np.max()), float(out_np.mean()))\n",
    "    after_tanh = np.tanh(out_np)\n",
    "    print(\"post-tanh min/max/mean:\", float(after_tanh.min()), float(after_tanh.max()), float(after_tanh.mean()))\n",
    "    # print last linear if any\n",
    "    last_linear = None\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last_linear = m\n",
    "    if last_linear is not None:\n",
    "        w = last_linear.weight.detach().cpu().numpy()\n",
    "        b = last_linear.bias.detach().cpu().numpy() if last_linear.bias is not None else None\n",
    "        print(\"last_linear weight mean/min/max:\", float(w.mean()), float(w.min()), float(w.max()))\n",
    "        if b is not None:\n",
    "            print(\"last_linear bias mean/min/max:\", float(b.mean()), float(b.min()), float(b.max()))\n",
    "    # image enc stats\n",
    "    if hasattr(model, \"img_enc\"):\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                f1 = model.img_enc(front_t)\n",
    "                f2 = model.img_enc(hand_t)\n",
    "            print(\"img feat f1 mean/min/max:\", float(f1.mean()), float(f1.min()), float(f1.max()))\n",
    "            print(\"img feat f2 mean/min/max:\", float(f2.mean()), float(f2.min()), float(f2.max()))\n",
    "        except Exception as e:\n",
    "            print(\"尝试获取 img_enc 特征时出错：\", e)\n",
    "    print(\"proprio (raw) stats min/max/mean:\", float(proprio_t.min()), float(proprio_t.max()), float(proprio_t.mean()))\n",
    "    if env is not None:\n",
    "        try:\n",
    "            print(\"env.action_space low:\", env.action_space.low, \" high:\", env.action_space.high)\n",
    "            low = env.action_space.low\n",
    "            high = env.action_space.high\n",
    "        except Exception:\n",
    "            low, high = -1.0, 1.0\n",
    "    else:\n",
    "        low, high = -1.0, 1.0\n",
    "    # mapping examples\n",
    "    action_a = low + (after_tanh + 1.0) * 0.5 * (high - low)\n",
    "    max_sym = np.maximum(np.abs(low), np.abs(high))\n",
    "    action_b = after_tanh * max_sym\n",
    "    print(\"mapped action A (env-range):\", action_a.astype(np.float32))\n",
    "    print(\"mapped action B (sym-debug):\", action_b.astype(np.float32))\n",
    "    # check if outputs are all positive/constant\n",
    "    if np.allclose(out_np, out_np[0]):\n",
    "        print(\"警告：输出所有元素相等，可能最后一层偏置过大或模型 collapse。\")\n",
    "    if np.all(out_np >= 0):\n",
    "        print(\"注意：原始输出全部为非负。请检查最后激活或输出尺度（是否需要 tanh/clip）。\")\n",
    "    # more model stats\n",
    "    print_model_stats(model)\n",
    "    return out_np, after_tanh, action_a, action_b\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--ckpt\", default=\"/home/wdy02/wdy_program/simulation_plus/IsaacLab/wdy_data/bc_data/bc_ckpts/ONE_PEG_IN_HOLE_60/best.pt\", help=\"模型 checkpoint 路径\")\n",
    "    p.add_argument(\"--scaler\", default=\"/home/wdy02/wdy_program/simulation_plus/IsaacLab/wdy_data/bc_data/bc_ckpts/ONE_PEG_IN_HOLE_60/scaler.pkl\", help=\"可选 scaler 路径（若有 Standardizer.load）\")\n",
    "    p.add_argument(\"--use_env\", action=\"store_true\", help=\"从环境获取观测（需要 IsaacSim 环境可用）\")\n",
    "    p.add_argument(\"--sample_npz\", default=\"/home/wdy02/software/isaacsim/wdy_data/bc_data/npz/ONE_PEG_IN_HOLE_60/demo_0.npz\", help=\"可选：从 npz 文件读取观测样本（优先）\")\n",
    "    p.add_argument(\"--num_samples\", type=int, default=1, help=\"要诊断的样本数\")\n",
    "    import sys as _sys\n",
    "    if 'ipykernel' in _sys.modules:\n",
    "        args, _ = p.parse_known_args()\n",
    "    else:\n",
    "        args = p.parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    state, model_kwargs, raw_ckpt = load_checkpoint(args.ckpt, device)\n",
    "    model = build_model_from_kwargs(model_kwargs, state, device)\n",
    "\n",
    "    # prepare a dummy sample for forward-check and possible adapter insertion\n",
    "    dummy_front = torch.zeros(1,3,128,128, device=device)\n",
    "    dummy_hand = torch.zeros(1,3,128,128, device=device)\n",
    "    # dummy proprio constructed from model_kwargs if possible, else fallback\n",
    "    proprio_len_guess = int(model_kwargs.get('input_dim') or model_kwargs.get('proprio_dim') or 10)\n",
    "    dummy_proprio = torch.zeros(1, proprio_len_guess, device=device)\n",
    "\n",
    "    # try to wrap model with adapter if needed (catches matmul mismatch)\n",
    "    try:\n",
    "        model, adapter_created = wrap_model_with_adapter_if_needed(model, dummy_front, dummy_hand, dummy_proprio, device)\n",
    "        if adapter_created:\n",
    "            print(\"注意：已自动插入 proprio adapter（临时解决维度不匹配）。建议修正 checkpoint.model_kwargs 或模型定义以匹配真实 proprio 维度。\")\n",
    "    except Exception as e:\n",
    "        print(\"尝试自动插入 adapter 失败，继续但你可能需要手动检查模型/输入维度。错误：\", e)\n",
    "\n",
    "    scaler = None\n",
    "    if args.scaler and Standardizer is not None:\n",
    "        try:\n",
    "            scaler = Standardizer.load(args.scaler)\n",
    "            print(\"Loaded scaler from\", args.scaler)\n",
    "        except Exception as e:\n",
    "            print(\"载入 scaler 失败:\", e)\n",
    "            scaler = None\n",
    "\n",
    "    env = None\n",
    "    if args.use_env:\n",
    "        if FrankaGym is None:\n",
    "            print(\"无法导入 FrankaGym，use_env 选项不可用。\")\n",
    "        else:\n",
    "            env = FrankaGym(render=False)\n",
    "            obs = env.reset()\n",
    "\n",
    "    # load samples\n",
    "    samples = []\n",
    "    if args.sample_npz:\n",
    "        if os.path.exists(args.sample_npz):\n",
    "            d = np.load(args.sample_npz, allow_pickle=True)\n",
    "            if 'obs' in d:\n",
    "                samples = list(d['obs'])\n",
    "            elif 'data' in d:\n",
    "                samples = list(d['data'])\n",
    "            elif 'observations' in d:\n",
    "                samples = list(d['observations'])\n",
    "            else:\n",
    "                sample = {}\n",
    "                for k in d.files:\n",
    "                    sample[k] = d[k]\n",
    "                samples = [sample]\n",
    "            print(f\"从 npz 读取到 {len(samples)} 个样本（优先使用）。\")\n",
    "        else:\n",
    "            print(\"sample_npz 不存在:\", args.sample_npz)\n",
    "\n",
    "    for i in range(args.num_samples):\n",
    "        if samples:\n",
    "            obs = samples[i % len(samples)]\n",
    "        elif env is not None:\n",
    "            obs = env.get_observation()\n",
    "        else:\n",
    "            obs = {\n",
    "                'agent_rgb': (np.random.rand(128,128,3)*255).astype(np.uint8),\n",
    "                'hand_rgb': (np.random.rand(128,128,3)*255).astype(np.uint8),\n",
    "                'ee_pose': np.zeros(7),\n",
    "                'joint_positions': np.zeros(7),\n",
    "                'joint_velocities': np.zeros(7),\n",
    "            }\n",
    "        print(f\"\\n--- sample {i+1} ---\")\n",
    "        try:\n",
    "            diagnose_once(model, obs, device, scaler=scaler, env=env)\n",
    "        except Exception as e:\n",
    "            print(\"诊断 sample 时出错：\", e)\n",
    "            # continue to next sample\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
